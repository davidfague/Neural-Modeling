{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../scripts/\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../Modules\")\n",
    "import analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store models\n",
    "models = {}\n",
    "\n",
    "# Path for Ben's model\n",
    "ben_base_path = \"L5BaselineResults/\"\n",
    "\n",
    "# Load data for 'ben' model\n",
    "models['ben'] = {\n",
    "    'v': np.array(h5py.File(ben_base_path + 'v_report.h5', 'r')['report']['biophysical']['data'][:50000,:]),\n",
    "    # 'hva': np.array(h5py.File(ben_base_path + 'Ca_HVA.ica_report.h5', 'r')['report']['biophysical']['data']),\n",
    "    # 'lva': np.array(h5py.File(ben_base_path + 'Ca_LVAst.ica_report.h5', 'r')['report']['biophysical']['data']),\n",
    "    # 'ih': np.array(h5py.File(ben_base_path + 'Ih.ihcn_report.h5', 'r')['report']['biophysical']['data']),\n",
    "    # 'nmda': np.array(h5py.File(ben_base_path + 'inmda_report.h5', 'r')['report']['biophysical']['data']),\n",
    "    # 'na': np.array(h5py.File(ben_base_path + 'NaTa_t.gNaTa_t_report.h5', 'r')['report']['biophysical']['data']),\n",
    "    'spktimes': h5py.File(ben_base_path + 'spikes.h5', 'r')['spikes']['biophysical']['timestamps'][:],\n",
    "    'spkinds': np.sort((h5py.File(ben_base_path + 'spikes.h5', 'r')['spikes']['biophysical']['timestamps'][:] * 10).astype(int)),\n",
    "    # 'na_df': pd.read_csv(ben_base_path + 'na.csv'),\n",
    "    # 'ca_df': pd.read_csv(ben_base_path + 'ca.csv'),\n",
    "    # 'nmda_df': pd.read_csv(ben_base_path + 'nmda.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the refactored model \n",
    "sim_directory = '2024-09-16-09-35-21-TuningBenInhSynapses/Complex_InhGmaxApic7.1_InhGmaxDend0.0016_SomaGmax0.0025_ExcGmax-1.0351_Np1000'\n",
    "#'2024-09-06-10-33-43-TuningBenInhSynapses/Complex_InhGmaxApic5.0_InhGmaxDend0.005_SomaGmax0.001_ExcGmax-1.0351_Np1000'\n",
    "#'2024-09-05-16-45-46-TuningBenInhSynapses/Complex_InhGmaxApic4.5_InhGmaxDend0.007_SomaGmax0.001_ExcGmax-1.0351_Np1000'\n",
    "#'2024-09-05-13-02-34-TuningBenInhSynapses/Complex_InhGmaxApic3.6_InhGmaxDend0.009_SomaGmax0.001_ExcGmax-1.0351_Np1000'\n",
    "#'2024-09-04-15-32-57-TuningBenInhSynapses/Complex_InhGmaxApic2.2_InhGmaxDend0.009_SomaGmax0.003_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-29-11-36-26-TuningSynapses_AfterUpdateExcRates_finalAdjustments/Complex_InhGmaxApic206.0_InhGmaxDend7.2_SomaGmax6.0_ExcGmax-1.0351_Np1000'\n",
    "#Complex_InhGmaxApic150.0_InhGmaxDend1.0_SomaGmax9.0_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-26-17-50-00-TuningSynapses_AfterUpdateExcRates_HighApicAndSomaInh/Complex_InhGmaxApic115.0_InhGmaxDend3.1_SomaGmax10.0_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-26-10-00-53-TuningSynapses_AfterUpdateExcRates/Complex_InhGmaxApic10.1_InhGmaxDend3.2_SomaGmax4.1_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-23-15-05-35-TuningSynapses_finalSims/Complex_InhGmaxApic6.9_InhGmaxDend0.1_SomaGmax2.3_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-19-15-18-27-TuningSynapses_finalSims/Complex_InhGmaxApic6.9_InhGmaxDend0.1_SomaGmax2.3_ExcGmax-1.0351_Np1000'\n",
    "#Complex_InhGmaxApic7.5_InhGmaxDend0.6_SomaGmax4.0_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-18-23-09-06-TuningSynapses_finalSims/Complex_InhGmaxApic6.2_InhGmaxDend1.0_SomaGmax2.3_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-18-13-30-55-TuningSynapses_finalSims/Complex_InhGmaxApic6.2_InhGmaxDend1.0_SomaGmax2.3_ExcGmax-1.0351_Np1000'\n",
    "#Complex_InhGmaxApic4.2_InhGmaxDend2.4_SomaGmax2.0_ExcGmax-1.0351_Np1000\n",
    "#Complex_InhGmaxApic4.0_InhGmaxDend2.0_SomaGmax1.8_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-16-10-47-53-TuningSynapses_finalSims/Complex_InhGmaxApic5.0_InhGmaxDend2.4_SomaGmax2.1_ExcGmax-1.0351_Np1000'\n",
    "#Complex_InhGmaxApic4.4_InhGmaxDend2.2_SomaGmax2.0_ExcGmax-1.0351_Np1000'\n",
    "#Complex_InhGmaxApic4.4_InhGmaxDend2.2_SomaGmax2.0_ExcGmax-1.0351_Np1000\n",
    "#'2024-08-14-22-02-42-TuningSynapses_finalSims/Complex_InhGmaxApic3.4_InhGmaxDend2.0_SomaGmax0.9_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-14-20-38-05-TuningSynapses_finalSims/Complex_InhGmaxApic3.0_InhGmaxDend2.0_SomaGmax0.55_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-14-20-38-05-TuningSynapses_finalSims/Complex_InhGmaxApic3.2_InhGmaxDend2.4_SomaGmax0.4_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-14-20-38-05-TuningSynapses_finalSims/Complex_InhGmaxApic2.7_InhGmaxDend2.4_SomaGmax0.4_ExcGmax-1.0351_Np1000'\n",
    "#Complex_InhGmaxApic3.2_InhGmaxDend2.4_SomaGmax0.4_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-14-20-38-05-TuningSynapses_finalSims/Complex_InhGmaxApic2.6_InhGmaxDend2.0_SomaGmax0.55_ExcGmax-1.0351_Np1000'\n",
    "#2024-08-14-20-38-05-TuningSynapses_finalSims/Complex_InhGmaxApic3.0_InhGmaxDend2.0_SomaGmax0.55_ExcGmax-1.0351_Np1000'\n",
    "#'2024-08-14-20-08-53-TuningSynapses_finalSims/Complex_InhGmax2.4_SomaGmax0.4_ExcGmax-1.0351_Np1000/'\n",
    "#'2024-08-14-19-33-53-TuningSynapses_ManySims/Complex_InhGmax2.4_SomaGmax0.4_ExcGmax-1.0351_Np1000/'\n",
    "#'2024-08-14-16-42-45-TuningSynapses_ManySims/Complex_InhGmax2.4_SomaGmax0.4_ExcGmax-1.0351_Np1000/' # this one was decent I tihink\n",
    "#'2024-08-13-18-53-06-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "#'2024-08-13-18-24-09-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "#'2024-08-13-16-34-16-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "#'2024-08-13-16-00-54-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "#'2024-08-13-15-40-48-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "\n",
    "#2024-08-13-15-04-15-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "#'2024-08-13-08-20-38-TuningSynapses_reduceNA_shiftExcGmaxBy20Percent/Complex_Np5'\n",
    "\n",
    "#\"/home/drfrbc/Neural-Modeling/scripts/2024-08-02-08-31-54-STA/Complex_Np5\"\n",
    "# /home/drfrbc/Neural-Modeling/scripts/2024-08-12-15-16-33-TuningSynapses_shiftExcGmaxBy10Percent\n",
    "models['refactored'] = {\n",
    "    'v': analysis.DataReader.read_data(sim_directory, \"v\").T,\n",
    "    # 'hva': analysis.DataReader.read_data(sim_directory, \"ica_Ca_HVA\").T,\n",
    "    # 'lva': analysis.DataReader.read_data(sim_directory, \"ica_Ca_LVAst\").T,\n",
    "    # 'ih': analysis.DataReader.read_data(sim_directory, \"ihcn_Ih\").T,\n",
    "    # 'nmda': analysis.DataReader.read_data(sim_directory, \"i_NMDA\").T,\n",
    "    # 'na': analysis.DataReader.read_data(sim_directory, \"gNaTa_t_NaTa_t\").T,\n",
    "    'spktimes': analysis.DataReader.read_data(sim_directory, \"soma_spikes\")[0][:],\n",
    "    'spkinds': np.sort((analysis.DataReader.read_data(sim_directory, \"soma_spikes\")[0][:] * 10).astype(int)),\n",
    "    # 'na_df': pd.read_csv(os.path.join(sim_directory, 'na.csv')),\n",
    "    # 'ca_df': pd.read_csv(os.path.join(sim_directory, 'ca.csv')),\n",
    "    # 'nmda_df': pd.read_csv(os.path.join(sim_directory, 'nmda.csv'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segment data for both models\n",
    "segs_ben = pd.read_csv(ben_base_path + 'Segments.csv')\n",
    "segs_degrees = pd.read_csv(ben_base_path + 'SegmentsDegrees.csv').groupby(['Type','Sec ID'])['Degrees'].max().reset_index()\n",
    "segs_ben['segmentID'] = segs_ben.index\n",
    "segs_ben = segs_ben.set_index(['Type','Sec ID']).join(segs_degrees.set_index(['Type','Sec ID'])).reset_index()\n",
    "segs_ben['Sec ID'] = segs_ben['Sec ID'].astype(int)\n",
    "segs_ben['X'] = segs_ben['X'].astype(float)\n",
    "segs_ben['Elec_distanceQ'] = 'None'\n",
    "segs_ben.loc[segs_ben.Type=='dend','Elec_distanceQ'] = pd.qcut(segs_ben.loc[segs_ben.Type=='dend','Elec_distance'], 10, labels=False)\n",
    "segs_ben.loc[segs_ben.Type=='apic','Elec_distanceQ'] = pd.qcut(segs_ben.loc[segs_ben.Type=='apic','Elec_distance'], 10, labels=False)\n",
    "\n",
    "segs_refactored = pd.read_csv(os.path.join(sim_directory, \"segment_data.csv\"))\n",
    "segs_refactored['Sec ID'] = segs_refactored['idx_in_section_type']\n",
    "segs_refactored['Type'] = segs_refactored['section']\n",
    "segs_refactored['Coord X'] = segs_refactored['pc_0']\n",
    "segs_refactored['Coord Y'] = segs_refactored['pc_1']\n",
    "segs_refactored['Coord Z'] = segs_refactored['pc_2']\n",
    "elec_dist = pd.read_csv(os.path.join(sim_directory, f\"elec_distance_{'soma'}.csv\"))\n",
    "segs_refactored['Elec_distance'] = elec_dist['25_active']\n",
    "elec_dist = pd.read_csv(os.path.join(sim_directory, f\"elec_distance_{'nexus'}.csv\"))\n",
    "segs_refactored['Elec_distance_nexus'] = elec_dist['25_active']\n",
    "Xs = []\n",
    "for seg in segs_refactored['seg']:\n",
    "    Xs.append(seg.split('(')[-1].split(')')[0])\n",
    "segs_refactored['X'] = Xs\n",
    "\n",
    "# continue\n",
    "segs_refactored['segmentID'] = segs_refactored.index\n",
    "\n",
    "segs_refactored['Sec ID'] = segs_refactored['Sec ID'].astype(int)\n",
    "segs_refactored['X'] = segs_refactored['X'].astype(float)\n",
    "segs_refactored['Elec_distanceQ'] = 'None'\n",
    "\n",
    "segs_refactored.loc[segs_refactored.Type=='dend','Elec_distanceQ'] = pd.qcut(segs_refactored.loc[segs_refactored.Type=='dend','Elec_distance'], 10, labels=False)\n",
    "segs_refactored.loc[segs_refactored.Type=='apic','Elec_distanceQ'] = pd.qcut(segs_refactored.loc[segs_refactored.Type=='apic','Elec_distance'], 10, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['ben']['v'][:, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift 3D coordinates to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 3D coordinates to identify matching segment indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot target segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(models[model]['v'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points=np.arange(0,50000)\n",
    "colors = ['r*', 'g*', 'b*', 'm*', 'y*', 'k*']\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(models['ben']['v'][time_points, 2523], colors[-1].split('*')[0])\n",
    "plt.ylim([-90, 10])\n",
    "plt.title(f'Ben Model - SOMA Voltage at index {2523}')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(models['refactored']['v'][time_points, 0], colors[-1].split('*')[0])\n",
    "plt.ylim([-90, 10])\n",
    "plt.title(f'Refactored Model - SOMA Voltage at index {0}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_segs_refactored = segs_refactored[segs_refactored['Type']=='soma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_segs_ben = segs_ben[segs_ben['Type']=='soma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_segs_ben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_segs_refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models['ben']['v'][:,2523])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to count the first occurrence of True in consecutive trues\n",
    "def count_first_true(arr):\n",
    "    return np.sum((arr > -10) & np.concatenate(([True], arr[:-1] <= -10)))\n",
    "\n",
    "# Count spikes in the 'ben' model\n",
    "ben_spike_count = count_first_true(models['ben']['v'][:, 2523])\n",
    "ben_spike_rate = ben_spike_count / (len(models['ben']['v'][:, 2523]) / 10000)\n",
    "print(round(ben_spike_rate,3))\n",
    "\n",
    "# Count spikes in the 'refactored' model\n",
    "refactored_spike_count = count_first_true(models['refactored']['v'][:, 0])\n",
    "refactored_spike_rate = refactored_spike_count / (len(models['refactored']['v'][:, 0]) / 10000)\n",
    "print(round(refactored_spike_rate, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.45) - 0.5 * np.log((0.35/0.45)**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_shift_and_apply(ben_segs, refactored_segs):\n",
    "    # Calculate the min and max for each coordinate of Ben's segments\n",
    "    ben_min_x = ben_segs['Coord X'].min()\n",
    "    ben_max_x = ben_segs['Coord X'].max()\n",
    "    ben_min_y = ben_segs['Coord Y'].min()\n",
    "    ben_max_y = ben_segs['Coord Y'].max()\n",
    "    ben_min_z = ben_segs['Coord Z'].min()\n",
    "    ben_max_z = ben_segs['Coord Z'].max()\n",
    "\n",
    "    # Calculate the min and max for each coordinate of the refactored segments\n",
    "    refactored_min_x = refactored_segs['Coord X'].min()\n",
    "    refactored_max_x = refactored_segs['Coord X'].max()\n",
    "    refactored_min_y = refactored_segs['Coord Y'].min()\n",
    "    refactored_max_y = refactored_segs['Coord Y'].max()\n",
    "    refactored_min_z = refactored_segs['Coord Z'].min()\n",
    "    refactored_max_z = refactored_segs['Coord Z'].max()\n",
    "\n",
    "    # Calculate the average shift needed for each axis\n",
    "    shift_x = ((ben_min_x - refactored_min_x) + (ben_max_x - refactored_max_x)) / 2\n",
    "    shift_y = ((ben_min_y - refactored_min_y) + (ben_max_y - refactored_max_y)) / 2\n",
    "    shift_z = ((ben_min_z - refactored_min_z) + (ben_max_z - refactored_max_z)) / 2\n",
    "\n",
    "    # Apply the shifts to the refactored segments using .loc to avoid SettingWithCopyWarning\n",
    "    refactored_segs.loc[:, 'Coord X'] += shift_x\n",
    "    refactored_segs.loc[:, 'Coord Y'] += shift_y\n",
    "    refactored_segs.loc[:, 'Coord Z'] += shift_z\n",
    "\n",
    "    print(f\"Shift applied to refactored segments: X={shift_x}, Y={shift_y}, Z={shift_z}\")\n",
    "    return refactored_segs\n",
    "\n",
    "def map_segments(ben_segs, refactored_segs):\n",
    "    segment_mapping = {}\n",
    "\n",
    "    # Loop through each segment in Ben's data\n",
    "    for i, row in ben_segs.iterrows():\n",
    "        ben_coords = np.array([row['Coord X'], row['Coord Y'], row['Coord Z']])\n",
    "        \n",
    "        # Calculate the distance from the current Ben's segment to all segments in the refactored data\n",
    "        refactored_coords = refactored_segs[['Coord X', 'Coord Y', 'Coord Z']].values\n",
    "        dists = distance.cdist([ben_coords], refactored_coords, 'euclidean').flatten()\n",
    "        \n",
    "        # Find the index of the closest segment in the refactored data\n",
    "        closest_index = np.argmin(dists)\n",
    "        \n",
    "        # Store the mapping (Ben's segment ID -> Refactored segment ID)\n",
    "        segment_mapping[row['segmentID']] = refactored_segs.iloc[closest_index]['segmentID']\n",
    "\n",
    "    return segment_mapping\n",
    "\n",
    "def plot_segments(ben_segs, refactored_segs, segment_mapping, indices, colors, title_suffix=\"\"):\n",
    "    # Calculate the axis limits\n",
    "    all_coords_x = ben_segs['Coord X'].tolist() + refactored_segs['Coord X'].tolist()\n",
    "    all_coords_y = ben_segs['Coord Y'].tolist() + refactored_segs['Coord Y'].tolist()\n",
    "    x_min, x_max = min(all_coords_x), max(all_coords_x)\n",
    "    y_min, y_max = min(all_coords_y), max(all_coords_y)\n",
    "\n",
    "    for i, segs in enumerate([ben_segs, refactored_segs]):\n",
    "        plt.figure()\n",
    "        plt.scatter(segs['Coord X'], segs['Coord Y'], s=0.1)\n",
    "        for j, ind in enumerate(indices):\n",
    "            if i == 1:\n",
    "                ind = segment_mapping[ind]\n",
    "            plt.plot(segs.loc[segs.segmentID.isin([ind]), 'Coord X'], \n",
    "                     segs.loc[segs.segmentID.isin([ind]), 'Coord Y'], colors[j])\n",
    "        \n",
    "        plt.title(f\"Ben's Segments {title_suffix}\" if i == 0 else f\"Refactored Segments {title_suffix}\")\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.show()\n",
    "\n",
    "def plot_voltage(models, segment_mapping, indices, colors, title_suffix=\"\", time_points=np.arange(0, 50000)):\n",
    "    colors = [color.split('*')[0] for color in colors]\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(models['ben']['v'][time_points, idx], colors[i])\n",
    "        plt.ylim([-90, 10])\n",
    "        plt.title(f'Ben Model - Voltage at index {idx} {title_suffix}')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(models['refactored']['v'][time_points, segment_mapping[idx]], colors[i])\n",
    "        plt.ylim([-90, 10])\n",
    "        plt.title(f'Refactored Model - Voltage at index {segment_mapping[idx]} {title_suffix}')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming segs_ben and segs_refactored are your dataframes\n",
    "segs_refactored = calculate_shift_and_apply(segs_ben, segs_refactored)\n",
    "\n",
    "# Filter for specific segment types\n",
    "apic_segs_ben = segs_ben[segs_ben['Type']=='apic']\n",
    "apic_segs_refactored = segs_refactored[segs_refactored['Type']=='apic']\n",
    "\n",
    "# Calculate shifts and apply them\n",
    "# apic_segs_refactored_shifted = calculate_shift_and_apply(apic_segs_ben, apic_segs_refactored)\n",
    "\n",
    "# Map segments\n",
    "apic_segment_mapping = map_segments(apic_segs_ben, apic_segs_refactored)\n",
    "\n",
    "# Plot segments\n",
    "apic_indices = [1500, 1400, 1900, 2000, 2500, 1800]\n",
    "colors = ['r*', 'g*', 'b*', 'm*', 'y*', 'k*']\n",
    "plot_segments(apic_segs_ben, apic_segs_refactored, apic_segment_mapping, apic_indices, colors, title_suffix=\"(Apical)\")\n",
    "\n",
    "# Plot voltage\n",
    "plot_voltage(models, apic_segment_mapping, apic_indices, colors, title_suffix=\"(Apical)\")\n",
    "\n",
    "# Now for dendritic segments\n",
    "dend_segs_ben = segs_ben[segs_ben['Type']=='dend']\n",
    "dend_segs_refactored = segs_refactored[segs_refactored['Type']=='dend']\n",
    "\n",
    "# Calculate shifts and apply them\n",
    "# dend_segs_refactored_shifted = calculate_shift_and_apply(dend_segs_ben, dend_segs_refactored)\n",
    "\n",
    "# Map segments\n",
    "dend_segment_mapping = map_segments(dend_segs_ben, dend_segs_refactored)\n",
    "\n",
    "# Plot segments\n",
    "dend_indices = [10, 20, 30, 40, 50, 60]\n",
    "plot_segments(dend_segs_ben, dend_segs_refactored, dend_segment_mapping, dend_indices, colors, title_suffix=\"(Dendritic)\")\n",
    "\n",
    "# Plot voltage\n",
    "plot_voltage(models, dend_segment_mapping, dend_indices, colors, title_suffix=\"(Dendritic)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 50000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['refactored']['v'][:, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['ben']['v'][:, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate an average error for a series of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "factor in standard deviation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding soma into the calculation and factor in median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score errors so that they do not need to be weighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z score the mean, median, std, correlation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import zscore, pearsonr\n",
    "\n",
    "def subset_segments(segs, seg_type):\n",
    "    return segs[segs['Type'] == seg_type]\n",
    "\n",
    "\n",
    "def calculate_soma_spike_rate(v_trace):\n",
    "    def count_first_true(arr):\n",
    "        return np.sum((arr > -10) & np.concatenate(([True], arr[:-1] <= -10)))\n",
    "    \n",
    "    spike_count = count_first_true(v_trace)\n",
    "    spike_rate = spike_count / (len(v_trace) / 10000)  # Assuming 10kHz sampling rate\n",
    "    return spike_rate\n",
    "\n",
    "def calculate_individual_errors(models, segment_mapping, indices):\n",
    "    mean_errors = []\n",
    "    std_errors = []\n",
    "    median_errors = []\n",
    "    corr_errors = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        # print(len(models['ben']['v'][:, idx]), len(models['refactored']['v'][:50001, segment_mapping[idx]]))\n",
    "        ben_trace = models['ben']['v'][:, idx]\n",
    "        refactored_trace = models['refactored']['v'][:50001, segment_mapping[idx]]\n",
    "        \n",
    "        ben_avg = np.mean(ben_trace)\n",
    "        refactored_avg = np.mean(refactored_trace)\n",
    "        \n",
    "        ben_std = np.std(ben_trace)\n",
    "        refactored_std = np.std(refactored_trace)\n",
    "        \n",
    "        ben_median = np.median(ben_trace)\n",
    "        refactored_median = np.median(refactored_trace)\n",
    "        \n",
    "        mean_errors.append(np.abs(ben_avg - refactored_avg))\n",
    "        std_errors.append(np.abs(ben_std - refactored_std))\n",
    "        median_errors.append(np.abs(ben_median - refactored_median))\n",
    "        \n",
    "        corr_coef, _ = pearsonr(ben_trace, refactored_trace[:-1])\n",
    "        corr_errors.append(1 - corr_coef)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.sum(mean_errors),\n",
    "        'std': np.sum(std_errors),\n",
    "        'median': np.sum(median_errors),\n",
    "        'corr': np.sum(corr_errors)\n",
    "    }\n",
    "\n",
    "def calculate_soma_firing_rate_error(ben_model, refactored_model, soma_index):\n",
    "    ben_soma_rate = calculate_soma_spike_rate(ben_model['v'][:, soma_index])\n",
    "    refactored_soma_rate = calculate_soma_spike_rate(refactored_model['v'][:, 0])  # Assuming the first column for soma\n",
    "    \n",
    "    firing_rate_error = np.abs(ben_soma_rate - refactored_soma_rate)\n",
    "    return firing_rate_error\n",
    "\n",
    "def collect_errors_for_zscore(sim_directory_base, ben_model, segs_ben, segs_refactored, apic_indices, dend_indices, soma_indices):\n",
    "    apic_errors = {'mean': [], 'std': [], 'median': [], 'corr': []}\n",
    "    dend_errors = {'mean': [], 'std': [], 'median': [], 'corr': []}\n",
    "    soma_errors = {'mean': [], 'std': [], 'median': [], 'corr': []}\n",
    "    soma_firing_rate_errors = []\n",
    "    sim_names = []\n",
    "    \n",
    "    apic_segs_ben = subset_segments(segs_ben, 'apic')\n",
    "    apic_segs_refactored = subset_segments(segs_refactored, 'apic')\n",
    "    dend_segs_ben = subset_segments(segs_ben, 'dend')\n",
    "    dend_segs_refactored = subset_segments(segs_refactored, 'dend')\n",
    "    soma_segs_ben = subset_segments(segs_ben, 'soma')\n",
    "    soma_segs_refactored = subset_segments(segs_refactored, 'soma')\n",
    "    \n",
    "    apic_segment_mapping = map_segments(apic_segs_ben, apic_segs_refactored)\n",
    "    dend_segment_mapping = map_segments(dend_segs_ben, dend_segs_refactored)\n",
    "    soma_segment_mapping = map_segments(soma_segs_ben, soma_segs_refactored)\n",
    "    \n",
    "    for sim_name in os.listdir(sim_directory_base):\n",
    "        sim_directory = os.path.join(sim_directory_base, sim_name)\n",
    "        if os.path.isdir(sim_directory):\n",
    "            print(f\"Collecting errors for simulation: {sim_name}\")\n",
    "            \n",
    "            models['refactored'] = {\n",
    "                'v': analysis.DataReader.read_data(sim_directory, \"v\").T,\n",
    "                'spktimes': analysis.DataReader.read_data(sim_directory, \"soma_spikes\")[0][:],\n",
    "                'spkinds': np.sort((analysis.DataReader.read_data(sim_directory, \"soma_spikes\")[0][:] * 10).astype(int)),\n",
    "            }\n",
    "            \n",
    "            apic_error = calculate_individual_errors(models, apic_segment_mapping, apic_indices)\n",
    "            dend_error = calculate_individual_errors(models, dend_segment_mapping, dend_indices)\n",
    "            soma_error = calculate_individual_errors(models, soma_segment_mapping, soma_indices)\n",
    "            soma_firing_rate_error = calculate_soma_firing_rate_error(models['ben'], models['refactored'], soma_indices[0])\n",
    "            \n",
    "            for key in apic_errors.keys():\n",
    "                apic_errors[key].append(apic_error[key])\n",
    "                dend_errors[key].append(dend_error[key])\n",
    "                soma_errors[key].append(soma_error[key])\n",
    "                \n",
    "            soma_firing_rate_errors.append(soma_firing_rate_error)\n",
    "            sim_names.append(sim_name)\n",
    "\n",
    "    return apic_errors, dend_errors, soma_errors, soma_firing_rate_errors, sim_names\n",
    "\n",
    "def find_best_simulation_with_zscore(sim_directory_base, ben_model, segs_ben, segs_refactored, apic_indices, dend_indices, soma_indices):\n",
    "    apic_errors, dend_errors, soma_errors, soma_firing_rate_errors, sim_names = collect_errors_for_zscore(\n",
    "        sim_directory_base, ben_model, segs_ben, segs_refactored, apic_indices, dend_indices, soma_indices\n",
    "    )\n",
    "    \n",
    "    total_z_scores = np.zeros(len(sim_names))\n",
    "    \n",
    "    for key in apic_errors.keys():\n",
    "        apic_z_scores = zscore(apic_errors[key])\n",
    "        dend_z_scores = zscore(dend_errors[key])\n",
    "        soma_z_scores = zscore(soma_errors[key])\n",
    "        \n",
    "        # Check for NaN in the z-scores and print the simulation name and error type\n",
    "        if np.isnan(apic_z_scores).any():\n",
    "            print(f\"NaN found in apic errors for simulation {key} at indices: {np.where(np.isnan(apic_z_scores))}\")\n",
    "        if np.isnan(dend_z_scores).any():\n",
    "            print(f\"NaN found in dend errors for simulation {key} at indices: {np.where(np.isnan(dend_z_scores))}\")\n",
    "        if np.isnan(soma_z_scores).any():\n",
    "            print(f\"NaN found in soma errors for simulation {key} at indices: {np.where(np.isnan(soma_z_scores))}\")\n",
    "        \n",
    "        # Skip adding to total_z_scores if NaN is present\n",
    "        if not (np.isnan(apic_z_scores).any() or np.isnan(dend_z_scores).any() or np.isnan(soma_z_scores).any()):\n",
    "            total_z_scores += 2*apic_z_scores + 2*dend_z_scores + soma_z_scores\n",
    "    \n",
    "    soma_firing_rate_z_scores = zscore(soma_firing_rate_errors)\n",
    "    \n",
    "    # Check for NaN in soma firing rate z-scores\n",
    "    if np.isnan(soma_firing_rate_z_scores).any():\n",
    "        print(f\"NaN found in soma firing rate errors at indices: {np.where(np.isnan(soma_firing_rate_z_scores))}\")\n",
    "    else:\n",
    "        total_z_scores += soma_firing_rate_z_scores  # Weighting soma firing rate error more heavily\n",
    "    \n",
    "    # Find the best simulation index and simulation name\n",
    "    best_index = np.argmin(total_z_scores)\n",
    "    best_simulation = sim_names[best_index]\n",
    "    best_error = total_z_scores[best_index]\n",
    "    \n",
    "    return best_simulation, best_error\n",
    "\n",
    "# Define the base directory containing all refactored simulation directories\n",
    "sim_directory_base = '2024-09-16-09-35-21-TuningBenInhSynapses'#'2024-08-29-11-36-26-TuningSynapses_AfterUpdateExcRates_finalAdjustments'#'2024-08-19-15-18-27-TuningSynapses_finalSims/'\n",
    "\n",
    "# Define the apical, dendritic, and soma segments for inspecting\n",
    "apic_indices = np.arange(1100, 2600, 100)#[1500, 1400, 1900, 2000, 2500, 1800]\n",
    "dend_indices = np.arange(10, 160, 10)#[10, 20, 30, 40, 50, 60]\n",
    "soma_indices = [2]\n",
    "\n",
    "# Find the best simulation using z-scored errors\n",
    "best_simulation, best_error = find_best_simulation_with_zscore(\n",
    "    sim_directory_base,\n",
    "    models['ben'],\n",
    "    segs_ben,\n",
    "    segs_refactored,\n",
    "    apic_indices,\n",
    "    dend_indices,\n",
    "    soma_indices\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'2024-09-09-11-26-53-TuningBenInhSynapses/Complex_InhGmaxApic5.9_InhGmaxDend0.003_SomaGmax0.0011_ExcGmax-1.0351_Np1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm_dist(gmax_mean, gmax_std, gmax_scalar, size, clip):\n",
    "\tval = np.random.lognormal(gmax_mean, gmax_std, size)\n",
    "\ts = gmax_scalar * float(np.clip(val, clip[0], clip[1]))\n",
    "\treturn s\n",
    "\n",
    "mean = np.random.lognormal(np.log(0.45) - 0.5 * np.log((0.35 / 0.45) ** 2 + 1))\n",
    "std = np.sqrt(np.log((0.35/0.45)**2 + 1))\n",
    "size = 1000\n",
    "\n",
    "vals = [log_norm_dist(mean, std, gmax_scalar=1, size=1, clip = [0, 5]) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
