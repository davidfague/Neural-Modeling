{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/drfrbc/Neural-Modeling/scripts\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../Modules/\")\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%cd ../scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_folder = '11-06-2024-11-56-10-STAs/Complex_Np5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.compare_sta import get_all_directories_within, group_directories_by_cell_and_seed\n",
    "sim_directory = '11-06-2024-11-56-10-STAs/'\n",
    "sim_directories = get_all_directories_within(sim_directory)\n",
    "grouped_directories = group_directories_by_cell_and_seed(sim_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get observation data (Each segment's voltage, synaptic currents, ion channel currents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gNaTa_t_NaTa_t' 'i_AMPA' 'i_NMDA' 'i_pas' 'ica' 'ica_Ca_HVA'\n",
      " 'ica_Ca_LVAst' 'ihcn_Ih' 'ik' 'ik_Im' 'ik_SK_E2' 'ik_SKv3_1' 'ina'\n",
      " 'ina_NaTa_t' 'ina_Nap_Et2' 'v']\n",
      "(643, 120001, 16)\n"
     ]
    }
   ],
   "source": [
    "parameters = analysis.DataReader.load_parameters(sim_folder)\n",
    "parameters.channel_names.extend(['v', 'i_AMPA', 'i_NMDA'])\n",
    "parameters.channel_names = list(np.unique(parameters.channel_names))\n",
    "\n",
    "print(parameters.channel_names)\n",
    "\n",
    "all_data_list = [analysis.DataReader.read_data(sim_folder, data_name).astype(np.float16) for data_name in parameters.channel_names]\n",
    "\n",
    "\n",
    "# Combine them into a single 3D array of shape (nseg, ntimes, ndatatypes)\n",
    "all_obs_data_matrix = np.stack(all_data_list, axis=-1)\n",
    "print(all_obs_data_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsegs = all_obs_data_matrix.shape[0]\n",
    "ntimes = all_obs_data_matrix.shape[1]\n",
    "n_obs_datatypes = all_obs_data_matrix.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get state data (Each segment's spike classification over time and Soma spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soma spike states\n",
    "soma_spikes = analysis.DataReader.read_data(sim_folder, 'soma_spikes')\n",
    "flat_soma_spikes = soma_spikes.flatten()\n",
    "# must round because soma spikes are recorded at dt=0.1 ms while all data is sampled every 1 ms\n",
    "rounded_indices = np.round(flat_soma_spikes).astype(int) # may need to drop duplicates\n",
    "\n",
    "# create the binary sequence\n",
    "# Determine the length of the binary sequence\n",
    "sequence_length = ntimes\n",
    "binary_sequence = np.zeros(sequence_length, dtype=int)\n",
    "binary_sequence[rounded_indices] = 1\n",
    "soma_spikes_binary = binary_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gNaTa_t_NaTa_t', 'i_AMPA', 'i_NMDA', 'i_pas', 'ica', 'ica_Ca_HVA',\n",
       "       'ica_Ca_LVAst', 'ihcn_Ih', 'ik', 'ik_Im', 'ik_SK_E2', 'ik_SKv3_1',\n",
       "       'ina', 'ina_NaTa_t', 'ina_Nap_Et2', 'v'], dtype='<U14')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.channel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3381168/1416304737.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  start_idx = int(np.round(start))\n",
      "/tmp/ipykernel_3381168/1416304737.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  end_idx = int(np.round(end))\n",
      "/tmp/ipykernel_3381168/1416304737.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  start_idx = int(np.round(start))\n",
      "/tmp/ipykernel_3381168/1416304737.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  end_idx = int(np.round(end))\n",
      "/tmp/ipykernel_3381168/1416304737.py:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  start_idx = int(np.round(start))\n",
      "/tmp/ipykernel_3381168/1416304737.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  end_idx = int(np.round(end))\n"
     ]
    }
   ],
   "source": [
    "# get dendritic spike states\n",
    "ica = all_obs_data_matrix[:, :, parameters.channel_names.index('ica')]\n",
    "v = all_obs_data_matrix[:, :, parameters.channel_names.index('v')]\n",
    "inmda = all_obs_data_matrix[:, :, parameters.channel_names.index('i_NMDA')]\n",
    "\n",
    "nseg = all_obs_data_matrix.shape[0]\n",
    "ntimes = all_obs_data_matrix.shape[1]\n",
    "nspike_types = 3\n",
    "\n",
    "# Initialize the 3D matrix\n",
    "spike_matrix = np.zeros((nseg, ntimes, nspike_types), dtype=int)\n",
    "\n",
    "for i in range(nseg):\n",
    "    # Get Ca spikes\n",
    "    left_bounds, right_bounds, _ = analysis.VoltageTrace.get_Ca_spikes(v[i], -40, ica[i])\n",
    "    # Convert Ca spikes to binary sequence\n",
    "    for start, end in zip(left_bounds, right_bounds):\n",
    "        start_idx = int(np.round(start))\n",
    "        end_idx = int(np.round(end))\n",
    "        spike_matrix[i, start_idx:end_idx+1, 0] = 1\n",
    "\n",
    "    # Get Na spikes and their durations\n",
    "    left_bounds, right_bounds, _ = analysis.VoltageTrace.get_Na_spikes(v[i], 0.001 / 1000, soma_spikes, 2, v[i], v[0])\n",
    "    # Convert NMDA spikes to binary sequence\n",
    "    for start, end in zip(left_bounds, right_bounds):\n",
    "        start_idx = int(np.round(start))\n",
    "        end_idx = int(np.round(end))\n",
    "        spike_matrix[i, start_idx:end_idx+1, 1] = 1\n",
    "    # threshold = 0.001 / 1000\n",
    "    # spikes, _ = analysis.VoltageTrace.get_Na_spikes(v[i], threshold, soma_spikes, 2, v[i], v[0])\n",
    "    # if len(spikes) > 0:\n",
    "    #     _, downward_crossing = analysis.VoltageTrace.get_crossings(v[i], threshold)\n",
    "    #     durations = analysis.VoltageTrace.get_duration(spikes, downward_crossing)\n",
    "    #     for spike_start, duration in zip(spikes, durations):\n",
    "    #         start_idx = int(np.round(spike_start))\n",
    "    #         end_idx = int(np.round(spike_start + duration))\n",
    "    #         spike_matrix[i, start_idx:end_idx+1, 1] = 1\n",
    "\n",
    "    # Get NMDA spikes\n",
    "    left_bounds, right_bounds, _ = analysis.VoltageTrace.get_NMDA_spikes(v[i], -40, inmda[i])\n",
    "    # Convert NMDA spikes to binary sequence\n",
    "    for start, end in zip(left_bounds, right_bounds):\n",
    "        start_idx = int(np.round(start))\n",
    "        end_idx = int(np.round(end))\n",
    "        spike_matrix[i, start_idx:end_idx+1, 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643, 120001, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ntimes where segment 449 had Ca spikes: 92238\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Segment index (449)\n",
    "segment_index = 449\n",
    "\n",
    "# Spike type index for Ca spikes (0)\n",
    "ca_spike_type_index = 0\n",
    "\n",
    "# Calculate the total ntimes where segment 449 had Ca spikes\n",
    "total_ca_spikes_ntimes = np.sum(spike_matrix[segment_index, :, ca_spike_type_index])\n",
    "\n",
    "print(f\"Total ntimes where segment {segment_index} had Ca spikes: {total_ca_spikes_ntimes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(643, 120001, 16)\n",
      "(643, 120001, 3)\n"
     ]
    }
   ],
   "source": [
    "print(all_obs_data_matrix.shape) # currents/conductances\n",
    "print(spike_matrix.shape) # spike occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate State Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Counts:\n",
      " [[3.9222683e+07 6.1213800e+05 1.6250000e+03 4.8000000e+01 1.1845300e+05\n",
      "  1.4489000e+05 1.2000000e+01 2.2000000e+01]\n",
      " [5.8402300e+05 2.5542380e+07 0.0000000e+00 3.9000000e+01 1.5800000e+02\n",
      "  1.5279400e+05 0.0000000e+00 0.0000000e+00]\n",
      " [5.1600000e+02 0.0000000e+00 5.1600000e+02 1.1020000e+03 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 7.0000000e+00]\n",
      " [0.0000000e+00 1.1640000e+03 0.0000000e+00 9.4000000e+02 0.0000000e+00\n",
      "  4.1000000e+01 0.0000000e+00 0.0000000e+00]\n",
      " [3.5607000e+04 2.1800000e+02 0.0000000e+00 0.0000000e+00 1.1434090e+06\n",
      "  9.1971000e+04 0.0000000e+00 0.0000000e+00]\n",
      " [2.5704200e+05 1.2346900e+05 0.0000000e+00 0.0000000e+00 9.1850000e+03\n",
      "  9.1154510e+06 0.0000000e+00 7.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.2000000e+01]\n",
      " [0.0000000e+00 2.5000000e+01 0.0000000e+00 1.6000000e+01 0.0000000e+00\n",
      "  7.0000000e+00 0.0000000e+00 3.0000000e+01]]\n",
      "Transition Probabilities:\n",
      " [[9.78124917e-01 1.52653359e-02 4.05238211e-05 1.19701133e-06\n",
      "  2.95394965e-03 3.61322858e-03 2.99252833e-07 5.48630194e-07]\n",
      " [2.22236099e-02 9.71954681e-01 0.00000000e+00 1.48405249e-06\n",
      "  6.01231520e-06 5.81421322e-03 0.00000000e+00 0.00000000e+00]\n",
      " [2.41008874e-01 0.00000000e+00 2.41008874e-01 5.14712751e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.26950023e-03]\n",
      " [0.00000000e+00 5.42657343e-01 0.00000000e+00 4.38228438e-01\n",
      "  0.00000000e+00 1.91142191e-02 0.00000000e+00 0.00000000e+00]\n",
      " [2.80104310e-02 1.71490830e-04 0.00000000e+00 0.00000000e+00\n",
      "  8.99468614e-01 7.23494637e-02 0.00000000e+00 0.00000000e+00]\n",
      " [2.70423814e-02 1.29896896e-02 0.00000000e+00 0.00000000e+00\n",
      "  9.66317852e-04 9.59000875e-01 0.00000000e+00 7.36442566e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 3.20512821e-01 0.00000000e+00 2.05128205e-01\n",
      "  0.00000000e+00 8.97435897e-02 0.00000000e+00 3.84615385e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# Assuming spike_matrix is shape (nseg, ntimes, nspike_types)\n",
    "nstates = 2 ** spike_matrix.shape[2]  # Total number of possible states\n",
    "nseg, ntimes, nspike_types = spike_matrix.shape\n",
    "\n",
    "# Initialize the transition count matrix\n",
    "transition_counts = np.zeros((nstates, nstates))\n",
    "\n",
    "# Function to convert state vector to a unique state index\n",
    "def state_vector_to_index(state_vector):\n",
    "    return sum(val * (2 ** idx) for idx, val in enumerate(state_vector))\n",
    "\n",
    "# Count transitions between states\n",
    "for seg in range(nseg):\n",
    "    for t in range(ntimes - 1):\n",
    "        current_state = state_vector_to_index(spike_matrix[seg, t])\n",
    "        next_state = state_vector_to_index(spike_matrix[seg, t + 1])\n",
    "        transition_counts[current_state, next_state] += 1\n",
    "\n",
    "# Normalize to get transition probabilities\n",
    "transition_probabilities = transition_counts / transition_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Handle cases where there are no transitions from a state to avoid division by zero\n",
    "transition_probabilities = np.nan_to_num(transition_probabilities)\n",
    "\n",
    "# Example printout to verify the matrix\n",
    "print(\"Transition Counts:\\n\", transition_counts)\n",
    "print(\"Transition Probabilities:\\n\", transition_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State 0 (000): No spikes of any type.\n",
    "State 1 (001): NMDA spikes only.\n",
    "State 2 (010): Sodium (Na) spikes only.\n",
    "State 3 (011): Sodium (Na) and NMDA spikes.\n",
    "State 4 (100): Calcium (Ca) spikes only.\n",
    "State 5 (101): Calcium (Ca) and NMDA spikes.\n",
    "State 6 (110): Calcium (Ca) and Sodium (Na) spikes.\n",
    "State 7 (111): Calcium (Ca), Sodium (Na), and NMDA spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "(8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(transition_counts.shape)\n",
    "print(transition_probabilities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: Collected 40100514 observations\n",
      "State 2: Collected 2141 observations\n",
      "State 3: Collected 2145 observations\n",
      "State 1: Collected 26279394 observations\n",
      "State 4: Collected 1271205 observations\n",
      "State 5: Collected 9505154 observations\n",
      "State 7: Collected 78 observations\n",
      "State 6: Collected 12 observations\n",
      "State 0: Observations shape (40100514, 16)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.78 GiB for an array with shape (40100514, 16) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m emission_probabilities\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate the emission probabilities\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m emission_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_emission_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_obs_data_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspike_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstates\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 33\u001b[0m, in \u001b[0;36mcalculate_emission_probabilities\u001b[0;34m(combined_data, state_data, nstates)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Observations shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservations\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     mean \u001b[38;5;241m=\u001b[39m observations\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     cov \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     emission_probabilities[state] \u001b[38;5;241m=\u001b[39m (mean, cov)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sim_env/lib/python3.10/site-packages/numpy/lib/function_base.py:2674\u001b[0m, in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2672\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(m, y, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m-> 2674\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rowvar \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2676\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 4.78 GiB for an array with shape (40100514, 16) and data type float64"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to convert state vector to unique index\n",
    "def state_vector_to_index(state_vector):\n",
    "    return sum(val * (2 ** idx) for idx, val in enumerate(state_vector))\n",
    "\n",
    "# Function to calculate emission probabilities\n",
    "def calculate_emission_probabilities(combined_data, state_data, nstates):\n",
    "    emissions = defaultdict(list)\n",
    "    \n",
    "    for seg in range(nseg):\n",
    "        for t in range(ntimes):\n",
    "            state_tuple = tuple(state_data[seg, t])\n",
    "            state_index = state_vector_to_index(state_tuple)\n",
    "            observations = combined_data[seg, t, :]\n",
    "            emissions[state_index].append(observations)\n",
    "    \n",
    "    # Debugging: Print collected emissions\n",
    "    for state, obs in emissions.items():\n",
    "        print(f\"State {state}: Collected {len(obs)} observations\")\n",
    "\n",
    "    # Calculate mean and covariance for each state\n",
    "    emission_probabilities = {}\n",
    "    for state in range(nstates):\n",
    "        if emissions[state]:\n",
    "            observations = np.array(emissions[state])\n",
    "            print(f\"State {state}: Observations shape {observations.shape}\")  # Debugging\n",
    "            mean = observations.mean(axis=0)\n",
    "            cov = np.cov(observations, rowvar=False)\n",
    "            emission_probabilities[state] = (mean, cov)\n",
    "        else:\n",
    "            print(f\"State {state}: No observations\")  # Debugging\n",
    "            emission_probabilities[state] = (np.zeros(combined_data.shape[2]), np.eye(combined_data.shape[2]))\n",
    "    \n",
    "    return emission_probabilities\n",
    "\n",
    "# Calculate the emission probabilities\n",
    "emission_probabilities = calculate_emission_probabilities(all_obs_data_matrix, spike_matrix, nstates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Helper function to convert state vector to unique index\n",
    "def state_vector_to_index(state_vector):\n",
    "    return sum(val * (2 ** idx) for idx, val in enumerate(state_vector))\n",
    "\n",
    "# Incremental mean and covariance calculation\n",
    "class IncrementalCovariance:\n",
    "    def __init__(self, n_features, dtype=np.float32):\n",
    "        self.n_features = n_features\n",
    "        self.dtype = dtype\n",
    "        self.mean = np.zeros(n_features, dtype=dtype)\n",
    "        self.covariance = np.zeros((n_features, n_features), dtype=dtype)\n",
    "        self.n_samples = 0\n",
    "\n",
    "    def update(self, x):\n",
    "        self.n_samples += 1\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta / self.n_samples\n",
    "        self.covariance += np.outer(delta, x - self.mean)\n",
    "\n",
    "    def finalize(self):\n",
    "        return self.mean, self.covariance / (self.n_samples - 1) if self.n_samples > 1 else self.covariance\n",
    "\n",
    "def calculate_emission_probabilities(combined_data, state_data, nstates):\n",
    "    dtype = np.float32  # Change data type here\n",
    "    emissions = defaultdict(list)\n",
    "    \n",
    "    for seg in range(nseg):\n",
    "        for t in range(ntimes):\n",
    "            state_tuple = tuple(state_data[seg, t])\n",
    "            state_index = state_vector_to_index(state_tuple)\n",
    "            observations = combined_data[seg, t, :].astype(dtype)\n",
    "            emissions[state_index].append(observations)\n",
    "    \n",
    "    # Debugging: Print collected emissions\n",
    "    for state, obs in emissions.items():\n",
    "        print(f\"State {state}: Collected {len(obs)} observations\")\n",
    "\n",
    "    # Calculate mean and covariance for each state\n",
    "    emission_probabilities = {}\n",
    "    for state in range(nstates):\n",
    "        if emissions[state]:\n",
    "            obs_iter = iter(emissions[state])\n",
    "            first_obs = next(obs_iter)\n",
    "            inc_cov = IncrementalCovariance(len(first_obs), dtype=dtype)\n",
    "            inc_cov.update(first_obs)\n",
    "            for observation in obs_iter:\n",
    "                inc_cov.update(observation)\n",
    "            mean, cov = inc_cov.finalize()\n",
    "            print(f\"State {state}: Mean shape {mean.shape}, Covariance shape {cov.shape}\")  # Debugging\n",
    "            emission_probabilities[state] = (mean, cov)\n",
    "        else:\n",
    "            print(f\"State {state}: No observations\")  # Debugging\n",
    "            emission_probabilities[state] = (np.zeros(combined_data.shape[2], dtype=dtype), np.eye(combined_data.shape[2], dtype=dtype))\n",
    "    \n",
    "    return emission_probabilities\n",
    "\n",
    "# Calculate the emission probabilities\n",
    "emission_probabilities = calculate_emission_probabilities(all_obs_data_matrix, spike_matrix, nstates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "print(emission_probabilities[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Transition and Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to get probability of an observation given a state\n",
    "def get_emission_probability(observation, state, emission_probabilities):\n",
    "    mean, cov = emission_probabilities[state]\n",
    "    return multivariate_normal.pdf(observation, mean=mean, cov=cov)\n",
    "\n",
    "# Example use of transition and emission probabilities for a given observation\n",
    "def infer_state(observation, previous_state, transition_probabilities, emission_probabilities):\n",
    "    max_prob = -1\n",
    "    best_next_state = None\n",
    "    for next_state in range(nstates):\n",
    "        trans_prob = transition_probabilities[previous_state, next_state]\n",
    "        emit_prob = get_emission_probability(observation, next_state, emission_probabilities)\n",
    "        prob = trans_prob * emit_prob\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            best_next_state = next_state\n",
    "    return best_next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLd below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'data' is a NumPy array of shape (n_samples, n_features)\n",
    "n_samples = 1000\n",
    "n_features = 7\n",
    "data = np.random.rand(n_samples, n_features)  # Replace with your actual data\n",
    "\n",
    "# Generate labels for each sample\n",
    "labels = np.random.choice(['CA_nexus', 'NMDA_distal', 'Soma_spike', 'Soma_burst'], n_samples)\n",
    "\n",
    "# Encode labels to integers\n",
    "# Encode labels to integersg\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "# Combine data and encoded labels into observation sequences\n",
    "observation_sequences = np.column_stack((data, encoded_labels))\n",
    "\n",
    "\n",
    "# Define the number of hidden states based on unique labels\n",
    "n_states = len(np.unique(encoded_labels))\n",
    "\n",
    "# Initialize the Gaussian HMM\n",
    "model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"diag\", n_iter=100)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(data)\n",
    "\n",
    "# Get the hidden states\n",
    "hidden_states = model.predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.predict(data)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(hidden_states, label='Hidden States')\n",
    "plt.title('Hidden State Sequence Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('State')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
